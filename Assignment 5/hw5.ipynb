{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLJgoWHXitJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc20734-e728-49f1-cbfe-a64e44fd0d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, None, 64)          4079616   \n",
            "                                                                 \n",
            " global_max_pooling1d_12 (Gl  (None, 64)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,079,876\n",
            "Trainable params: 4,079,876\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.5855 - accuracy: 0.8359 - val_loss: 0.3276 - val_accuracy: 0.8937\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2682 - accuracy: 0.9155 - val_loss: 0.2778 - val_accuracy: 0.9074\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2024 - accuracy: 0.9340 - val_loss: 0.2646 - val_accuracy: 0.9107\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1591 - accuracy: 0.9479 - val_loss: 0.2642 - val_accuracy: 0.9093\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1278 - accuracy: 0.9568 - val_loss: 0.2721 - val_accuracy: 0.9075\n",
            "\n",
            "Test loss: 0.25974202156066895\n",
            "Test accuracy: 0.9161841869354248\n"
          ]
        }
      ],
      "source": [
        "#Danny Hong\n",
        "#ECE 472 - Deep Learning\n",
        "#Assignment 5\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dropout, GlobalMaxPool1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def get_data(train_csv_file, test_csv_file):\n",
        "\n",
        "  train, test = pd.read_csv(train_csv_file), pd.read_csv(test_csv_file)\n",
        "\n",
        "  train['text'], test['text'] = (train.Title + \" \" + train.Description), (test.Title + \" \" + test.Description)\n",
        "\n",
        "  #Encoding the target labels with values between 0 and num_classes - 1 (in this case from 0 to 3).\n",
        "  label_encoder = LabelEncoder().fit(train['Class Index'])\n",
        "  train['label'], test['label'] = label_encoder.transform(train['Class Index']), label_encoder.transform(test['Class Index'])\n",
        "\n",
        "  return train, test\n",
        "\n",
        "num_classes = 4\n",
        "split_fraction = 0.8\n",
        "\n",
        "def main():\n",
        "\n",
        "  #AG News Dataset was downloaded from this link: https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "  #The train and test files were then uploaded into the notebook directory and read in.\n",
        "  train, test = get_data(train_csv_file = \"train.csv\", test_csv_file = \"test.csv\")\n",
        "\n",
        "  #Applying cross validation by splitting each of the training sets into a new training set and a validation set\n",
        "  original_train_size = len(train)\n",
        "  new_train_size = int(split_fraction * original_train_size)\n",
        "  val = train[new_train_size:original_train_size]\n",
        "  train = train[0:new_train_size]\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "  dictionary_size = len(tokenizer.word_index)\n",
        "\n",
        "  val_encoded, train_encoded, test_encoded = tokenizer.texts_to_sequences(val.text), tokenizer.texts_to_sequences(train.text), tokenizer.texts_to_sequences(test.text)\n",
        "\n",
        "  x_val, x_train, x_test = pad_sequences(val_encoded), pad_sequences(train_encoded), pad_sequences(test_encoded)\n",
        "  y_val, y_train, y_test = val.label, train.label, test.label\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim = dictionary_size, output_dim = 64))\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(num_classes, activation = tf.nn.softmax))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  optimizer = Adam(learning_rate = 0.001 , decay = 0, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08)\n",
        "  model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, validation_data = (x_val, y_val), batch_size = 64, epochs = 5, verbose = 1)\n",
        "\n",
        "  score = model.evaluate(x_test, y_test, verbose = 0)\n",
        "  print('\\nTest loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "main()"
      ]
    }
  ]
}
