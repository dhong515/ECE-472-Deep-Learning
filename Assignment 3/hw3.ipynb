{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Danny Hong\n",
        "# ECE 472 - Deep Learning\n",
        "# Assignment 3\n",
        "\n",
        "import gzip\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense\n",
        "\n",
        "def get_x_data(x_data_file):\n",
        "  x_data = gzip.open(x_data_file, 'rb')\n",
        "  x_data.read(16)\n",
        "  x_data = np.frombuffer(x_data.read(), dtype = np.uint8).astype(np.float32).reshape(-1, num_rows, num_columns, 1)\n",
        "  x_data = x_data / 255\n",
        "\n",
        "  return x_data\n",
        "\n",
        "def get_y_data(y_data_file):\n",
        "  y_data = gzip.open(y_data_file, 'rb')\n",
        "  y_data.read(8)\n",
        "  y_data = np.frombuffer(y_data.read(), dtype = np.uint8).astype(np.float32)\n",
        "  y_data = to_categorical(y_data)\n",
        "\n",
        "  return y_data\n",
        "\n",
        "num_rows = 28\n",
        "num_columns = 28\n",
        "num_classes = 10\n",
        "weight_decay = 0.001\n",
        "\n",
        "def main():\n",
        "\n",
        "  #MNIST dataset was downloaded from this link: https://deepai.org/dataset/mnist\n",
        "  #Each of the files were then uploaded into the notebook directory and read in\n",
        "  x_train, x_test = get_x_data(x_data_file = \"train-images-idx3-ubyte.gz\"), get_x_data(x_data_file = \"t10k-images-idx3-ubyte.gz\")\n",
        "  y_train, y_test = get_y_data(y_data_file = \"train-labels-idx1-ubyte.gz\"), get_y_data(y_data_file = \"t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "  #Applying cross validation by splitting each of the training sets into a new training set and a validation set\n",
        "  original_train_size = len(x_train)\n",
        "  new_train_size = len(x_train) - len(x_test)\n",
        "  x_val, y_val = x_train[new_train_size:original_train_size], y_train[new_train_size:original_train_size]\n",
        "  x_train, y_train = x_train[0:new_train_size], y_train[0:new_train_size]\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = tf.nn.relu, input_shape = (num_rows, num_columns, 1)))\n",
        "  model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "  model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = tf.nn.relu))\n",
        "  model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation = tf.nn.relu, kernel_regularizer = l2(weight_decay)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation = tf.nn.softmax))\n",
        "\n",
        "  model.summary() \n",
        "\n",
        "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, epochs = 5, batch_size = 128, validation_data = (x_val, y_val), verbose = 1)\n",
        "  \n",
        "  score = model.evaluate(x_test, y_test, verbose = 0)\n",
        "  print('\\nTest loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjXZGwKiMaPf",
        "outputId": "b9132e9b-b8d1-4b63-bee4-5d1421c41256"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 41s 103ms/step - loss: 0.4890 - accuracy: 0.8904 - val_loss: 0.1839 - val_accuracy: 0.9779\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 40s 101ms/step - loss: 0.2028 - accuracy: 0.9666 - val_loss: 0.1342 - val_accuracy: 0.9824\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1564 - accuracy: 0.9731 - val_loss: 0.1077 - val_accuracy: 0.9863\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1331 - accuracy: 0.9767 - val_loss: 0.0937 - val_accuracy: 0.9886\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 40s 103ms/step - loss: 0.1238 - accuracy: 0.9787 - val_loss: 0.0909 - val_accuracy: 0.9890\n",
            "\n",
            "Test loss: 0.08305571973323822\n",
            "Test accuracy: 0.9894000291824341\n"
          ]
        }
      ]
    }
  ]
}