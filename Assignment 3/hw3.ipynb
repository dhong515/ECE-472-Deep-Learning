{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Danny Hong\n",
        "# ECE 472 - Deep Learning\n",
        "# Assignment 3\n",
        "\n",
        "import gzip\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense\n",
        "\n",
        "#I want to give thanks to this link for helping me understand how to upload and read in the training and test data files\n",
        "#so that I would be able to set up my get_x_data() ad get_y_data() functions. \n",
        "#https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python\n",
        "\n",
        "#I also want to thank Ravindra for helping me set up my CNN \n",
        "#In addition, this link also helped with that: \n",
        "#https://github.com/nikhilroxtomar/Deep-Learning-with-MNIST/blob/master/02%20-%20mnist_conv.py\n",
        "\n",
        "def get_x_data(x_data_file):\n",
        "  x_data = gzip.open(x_data_file, 'rb')\n",
        "  x_data.read(16)\n",
        "  x_data = np.frombuffer(x_data.read(), dtype = np.uint8).astype(np.float32).reshape(-1, num_rows, num_columns, 1)\n",
        "  x_data = x_data / 255\n",
        "\n",
        "  return x_data\n",
        "\n",
        "def get_y_data(y_data_file):\n",
        "  y_data = gzip.open(y_data_file, 'rb')\n",
        "  y_data.read(8)\n",
        "  y_data = np.frombuffer(y_data.read(), dtype = np.uint8).astype(np.float32)\n",
        "  y_data = tf.keras.utils.to_categorical(y_data)\n",
        "\n",
        "  return y_data\n",
        "\n",
        "np.random.seed(31415)\n",
        "num_rows = 28\n",
        "num_columns = 28\n",
        "num_classes = 10\n",
        "\n",
        "def main():\n",
        "\n",
        "  #MNIST dataset was downloaded from this link: https://deepai.org/dataset/mnist\n",
        "  #Each of the files were then uploaded into the notebook directory and read in\n",
        "  x_train, x_test = get_x_data(x_data_file = \"train-images-idx3-ubyte.gz\"), get_x_data(x_data_file = \"t10k-images-idx3-ubyte.gz\")\n",
        "  y_train, y_test = get_y_data(y_data_file = \"train-labels-idx1-ubyte.gz\"), get_y_data(y_data_file = \"t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "  #Applying cross validation by splitting each of the training sets into a new training set and a validation set\n",
        "  original_train_size = len(x_train)\n",
        "  new_train_size = len(x_train) - len(x_test)\n",
        "  x_val, y_val = x_train[new_train_size:original_train_size], y_train[new_train_size:original_train_size]\n",
        "  x_train, y_train = x_train[0:new_train_size], y_train[0:new_train_size]\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = tf.nn.relu, input_shape = (num_rows, num_columns, 1)))\n",
        "  model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "  model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = tf.nn.relu))\n",
        "  model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation = tf.nn.relu))\n",
        "  model.add(Dropout(0.50))\n",
        "  model.add(Dense(num_classes, activation = tf.nn.softmax, kernel_regularizer = l2(0.001)))\n",
        "\n",
        "  model.summary() \n",
        "\n",
        "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, epochs = 5, batch_size = 128, validation_data = (x_val, y_val), verbose = 1)\n",
        "  \n",
        "  score = model.evaluate(x_test, y_test, verbose = 0)\n",
        "  print('\\nTest loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjXZGwKiMaPf",
        "outputId": "92a34dd5-3a39-4d89-dc73-566b3fd02c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_41 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  (None, 13, 13, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  (None, 5, 5, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.3333 - accuracy: 0.9043 - val_loss: 0.0916 - val_accuracy: 0.9781\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.1188 - accuracy: 0.9701 - val_loss: 0.0677 - val_accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 37s 94ms/step - loss: 0.0888 - accuracy: 0.9778 - val_loss: 0.0594 - val_accuracy: 0.9861\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.0745 - accuracy: 0.9818 - val_loss: 0.0534 - val_accuracy: 0.9897\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.0652 - accuracy: 0.9841 - val_loss: 0.0492 - val_accuracy: 0.9891\n",
            "\n",
            "Test loss: 0.042354267090559006\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    }
  ]
}